{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1L4UegvdaNEmpmjQa7HCW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Work in Progress (WIP)"
      ],
      "metadata": {
        "id": "SOKqKtlO_FYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Beautiful Soup\n",
        "\n",
        "Beautiful Soup is a Python package for parsing HTML and XML documents. Webscrapping normally involves several steps and they include: \n",
        "\n",
        "*   Using the [requests](https://pypi.org/project/requests/) library to get the HTML of a page \n",
        "*   Using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to turn that into a soup object\n",
        "*   Applying find/find_all or select to find a particular HTML tag or CSS \n",
        "*   And, sometimes, some sort of iteration (e.g., for loop) to capture an element, such as finding all the hyperlinks on a page \n",
        "* Appending things to a list, opening links, and scrapping more content\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1_6ZliJ7V4wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Applications for BeautifulSoup and Webscrapping \n",
        "\n",
        "*   Monitoring e-commerce prices \n",
        "*   Analyzing social media web data\n",
        "*   Getting text for NLP analysis\n",
        "*   Extracting abstracts, key words, contact information\n",
        "*   Alternative to getting data where there is no API\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dSubmGoZObR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Things to Keep in Mind \n",
        "\n",
        " \n",
        "* Keep it simple and only scrape what you need. \n",
        "* Practice being a good internet citizen. Not all websites take kindly to scraping, and some may prohibit it explicitly. Check with the website owners if they're okay with scraping or see if there is a robots.txt file.\n",
        "* If you feel like your requests are excessive, add [time](https://docs.python.org/3/library/time.html) delays to your requests. \n",
        "* Websites can be very complex and, in my experience, websites with a lot of javascript can be impossible to scrape; sometime it just won't work. \n",
        "* All HTML is not created equal and some sites are not always consistant with their tagging, make sure to check \"View Page Source\" or \"Inspect\" as you go. "
      ],
      "metadata": {
        "id": "RM8PLGsea7Mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BeautifulSoup Doucmentation\n",
        "\n",
        "Read the Docs: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
      ],
      "metadata": {
        "id": "L02dx2Jwg_BX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the HTML"
      ],
      "metadata": {
        "id": "-K5w96-AhBw3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmhkPohqIeEo"
      },
      "outputs": [],
      "source": [
        "# install libraries \n",
        "\n",
        "!pip install requests \n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries \n",
        "\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests"
      ],
      "metadata": {
        "id": "iRdja7mpI_mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get(\"https://keithgalli.github.io/web-scraping/example.html\")\n",
        "\n",
        "# making into soup object \n",
        "page_html = page.text\n",
        "soup =bs(page_html, \"html.parser\")\n",
        "\n",
        "# alternatively \n",
        "# soup = bs(r.content)\n",
        "\n",
        "# # print page or pretty print \n",
        "# # print(soup)\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCrwwVL2JhU3",
        "outputId": "4fb74ddd-2be1-4e17-a071-d8442683682b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html>\n",
            " <head>\n",
            "  <title>\n",
            "   HTML Example\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <div align=\"middle\">\n",
            "   <h1>\n",
            "    HTML Webpage\n",
            "   </h1>\n",
            "   <p>\n",
            "    Link to more interesting example:\n",
            "    <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">\n",
            "     keithgalli.github.io/web-scraping/webpage.html\n",
            "    </a>\n",
            "   </p>\n",
            "  </div>\n",
            "  <h2>\n",
            "   A Header\n",
            "  </h2>\n",
            "  <p>\n",
            "   <i>\n",
            "    Some italicized text\n",
            "   </i>\n",
            "  </p>\n",
            "  <h2>\n",
            "   Another header\n",
            "  </h2>\n",
            "  <p id=\"paragraph-id\">\n",
            "   <b>\n",
            "    Some bold text\n",
            "   </b>\n",
            "  </p>\n",
            " </body>\n",
            "</html>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find and Find All \n",
        "* find: first element\n",
        "* find all: all elements related to a html tag (e.g., all H2s)"
      ],
      "metadata": {
        "id": "xWT1pvWJbKeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "header = soup.find(\"h2\")\n",
        "headers = soup.find_all(\"h2\")\n",
        "print(header)\n",
        "print(headers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqsU8PqJT5Y9",
        "outputId": "a30a4d2f-6726-4724-85c8-7ad275858bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<h2>A Header</h2>\n",
            "[<h2>A Header</h2>, <h2>Another header</h2>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass in a list of elements to look for\n",
        "h1_h2 = soup.find_all([\"h1\", \"h2\"])\n",
        "print(h1_h2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBV0cRnE_uXt",
        "outputId": "601b0fab-7dd9-4d1e-99e7-a4e822f34578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<h1>HTML Webpage</h1>, <h2>A Header</h2>, <h2>Another header</h2>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use find/find all when looking for a particular attibute \n",
        "\n",
        "paragraph = soup.find_all(\"p\", attrs={\"id\": \"paragraph-id\"})\n",
        "print(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtS6j4w2yXI5",
        "outputId": "eae12991-24df-48f6-cace-bad34ecee8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<p id=\"paragraph-id\"><b>Some bold text</b></p>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nesting find and find all calls \n",
        "\n",
        "body = soup.find(\"body\")\n",
        "# body\n",
        "\n",
        "div = body.find(\"div\")\n",
        "#  div\n",
        "\n",
        "para = div.find(\"p\")\n",
        "para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMI72lUPx7ls",
        "outputId": "d3e39f57-37b8-4282-db2d-c8d03094c6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sesrch for specific strings in find/find all calls \n",
        "\n",
        "import re\n",
        "some_paragraphs = soup.find_all(\"p\", string=re.compile(\"Some\"))\n",
        "some_paragraphs\n",
        "\n",
        "# headers \n",
        "\n",
        "headers = soup.find_all(\"h2\", string = re.compile(\"(H|h)eader\"))\n",
        "headers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUcoNtgczv_r",
        "outputId": "3cb4a7af-b43f-4cf5-88ad-0d2a08879e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<h2>A Header</h2>, <h2>Another header</h2>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select (CSS Selector)\n",
        "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors\n",
        "CSS Selector Refereance: https://www.w3schools.com/cssref/css_selectors.php"
      ],
      "metadata": {
        "id": "gQ9XmSqm2RIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.body.prettify())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiNI46wt2P16",
        "outputId": "5211deb9-27d9-49c1-b061-2e323f16f9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<body>\n",
            " <div align=\"middle\">\n",
            "  <h1>\n",
            "   HTML Webpage\n",
            "  </h1>\n",
            "  <p>\n",
            "   Link to more interesting example:\n",
            "   <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">\n",
            "    keithgalli.github.io/web-scraping/webpage.html\n",
            "   </a>\n",
            "  </p>\n",
            " </div>\n",
            " <h2>\n",
            "  A Header\n",
            " </h2>\n",
            " <p>\n",
            "  <i>\n",
            "   Some italicized text\n",
            "  </i>\n",
            " </p>\n",
            " <h2>\n",
            "  Another header\n",
            " </h2>\n",
            " <p id=\"paragraph-id\">\n",
            "  <b>\n",
            "   Some bold text\n",
            "  </b>\n",
            " </p>\n",
            "</body>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Ways to Navigate "
      ],
      "metadata": {
        "id": "FC7gy31h-3ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup.select(\"div p\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDGH1wKdjEnc",
        "outputId": "75777807-d245-436a-b694-20a40ae07b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "paragraphs = soup.select(\"div ~ p\") "
      ],
      "metadata": {
        "id": "ITBWsMvUjU9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bold_text = soup.selct(\"paragraph-id b\")"
      ],
      "metadata": {
        "id": "tplK4MxdhxoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = soup.select(\"body > p\")"
      ],
      "metadata": {
        "id": "J1bnKZebhoiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for paragraph in paragraphs: \n",
        "  print(paragraph.select(\"i\"))"
      ],
      "metadata": {
        "id": "ALseM668h4z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# getting different properties of the HTML"
      ],
      "metadata": {
        "id": "qBwJkNntib9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heater = soup.find(\"h2\")\n",
        "heater.string\n",
        "\n",
        "div = soup.find(\"div\")\n",
        "print(print.prettify())\n",
        "print(div.string)\n",
        "\n",
        "# if the string gives \"none\", use get_text \n",
        "\n",
        "div = soup.find(\"div\")\n",
        "print(print.prettify())\n",
        "print(div.get_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hA10aZeUiUD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a spefific property from the element "
      ],
      "metadata": {
        "id": "7UtOc6Qxi_aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lnk = soup.find(\"a\")\n",
        "link[\"href\"]\n",
        "\n",
        "paragraph = soup.select(\"paragraph-id\")\n",
        "paragraph[0][\"id\"]"
      ],
      "metadata": {
        "id": "r2oZlgayjFah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path syntax \n",
        "\n",
        "soup\n",
        "\n",
        "soup.body \n",
        "\n",
        "soup.body.div.h1 \n",
        "\n",
        "soup.body.div.h1.string\n"
      ],
      "metadata": {
        "id": "vHaoaCTmjUER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# know the terms: parent, sibling, and child \n",
        "\n",
        "print(soup.body.prettify())\n",
        "\n",
        "# body is the parent, div is child and h1 is on the same level (sibling)\n",
        "# Review \"Navigating the tree\" in bs4 documentation \n",
        "\n",
        "soup.body.find(\"div\").find_next_siblings()\n"
      ],
      "metadata": {
        "id": "ZTyxHKozje1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises \n",
        "\n",
        "go to: https://keithgalli.github.io/web-scraping/webpage.html\n",
        "\n",
        "Check out the \"inspect\" to see the CSS"
      ],
      "metadata": {
        "id": "3HtxT-HRUY0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get(\"https://keithgalli.github.io/web-scraping/webpage.html\")\n",
        "\n",
        "#convert to a beautifulsoup object\n",
        "webpage = bs(r.content)\n",
        "print(webpage.prettify())\n",
        "\n",
        "# grab all of the social links from the webpage \n",
        "# you have to do with in at least three different ways, one was to use find/findall and the other use the select method \n",
        "\n"
      ],
      "metadata": {
        "id": "-a-IAp7XkPK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# "
      ],
      "metadata": {
        "id": "QUutvhPJUdXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# "
      ],
      "metadata": {
        "id": "C2C6mrW0Udfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteration / for loops"
      ],
      "metadata": {
        "id": "jYR3fBmchopk"
      }
    }
  ]
}